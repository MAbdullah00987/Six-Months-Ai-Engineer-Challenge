
Web Scraper Class Project
Web Scraper Class: Encapsulate web scraping logic into a reusable class

üìã Project Overview
A Web Scraper Class is a reusable Python tool that automatically extracts data from websites. Instead of manually copying information from web pages, you create a class that does it programmatically.
Real-world use cases:

Extracting product prices from e-commerce sites
Gathering news headlines
Collecting job listings
Monitoring competitor websites
Aggregating data for analysis


üîß Step-by-Step Breakdown
Step 1: Understanding the Concept
A class in Python is like a blueprint for creating objects. For web scraping, you'll create a class that:

Sends HTTP requests to websites
Downloads HTML content
Parses (analyzes) the HTML
Extracts specific data
Stores or returns the data

Step 2: Required Libraries
You'll need these Python libraries:
bashpip install requests beautifulsoup4 lxml
```

**What each library does:**
- **requests**: Sends HTTP requests to download web pages
- **beautifulsoup4**: Parses HTML and extracts data easily
- **lxml**: Fast HTML/XML parser (BeautifulSoup uses it)

### **Step 3: Class Structure**

Here's how the class architecture works:
```
WebScraper Class
‚îú‚îÄ‚îÄ __init__() - Initialize with URL and settings
‚îú‚îÄ‚îÄ fetch_page() - Download the webpage
‚îú‚îÄ‚îÄ parse_html() - Parse the HTML content
‚îú‚îÄ‚îÄ extract_data() - Extract specific elements
‚îú‚îÄ‚îÄ save_data() - Save to file/database
‚îî‚îÄ‚îÄ scrape() - Main method that orchestrates everything

üíª How Python Works in This Project
1. Object-Oriented Programming (OOP)
Python's OOP features make this project work:
pythonclass WebScraper:
    def __init__(self, url):
        self.url = url  # Instance variable
        self.html = None
        self.data = []

__init__: Constructor that runs when you create a scraper object
self: Refers to the instance itself (like "this" in other languages)
Instance variables: Store data specific to each scraper instance

2. HTTP Requests
pythonimport requests

response = requests.get(url)
# Python sends GET request to server
# Server responds with HTML content
# response.text contains the HTML
What happens:

Python's requests library opens a TCP connection
Sends HTTP GET request with headers
Receives response (status code + HTML)
Stores content in memory

3. HTML Parsing
pythonfrom bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'lxml')
# Creates a parse tree from HTML
# Allows easy navigation and searching
How it works:

BeautifulSoup converts HTML string into a tree structure
Each HTML tag becomes a Python object
You can search using CSS selectors or tag names

4. Data Extraction
python# Find elements by CSS selector
titles = soup.select('.product-title')

# Find by tag name
links = soup.find_all('a')

# Extract text or attributes
for title in titles:
    text = title.get_text()  # Gets the text inside tag
    href = title.get('href')  # Gets href attribute

Detailed Code Explanation
Key Methods Breakdown:
1. __init__() - Constructor
pythondef __init__(self, base_url, headers=None, delay=1.0):

Runs when you create an instance: scraper = WebScraper("https://example.com")
Sets up initial configuration
Creates a requests session (reuses connection for efficiency)

2. fetch_page() - Download HTML
pythonresponse = self.session.get(target_url, timeout=10)

Timeout: Prevents hanging if server is slow
Retry logic: Tries up to 3 times if it fails
Exponential backoff: Waits 1s, 2s, 4s between retries
Delay: Respects server by waiting between requests

3. parse_html() - Create Parse Tree
pythonself.soup = BeautifulSoup(html, 'lxml')

Converts raw HTML string into navigable tree structure
lxml is faster than default parser

4. extract_data() - Get Specific Elements
pythonelements = self.soup.select(selector)

Uses CSS selectors (same as in web development)
Examples: .class, #id, tag.class, div > p

5. extract_structured_data() - Complex Extraction
pythonconfig = {
    'title': {'selector': 'h2.title', 'attribute': 'text'},
    'link': {'selector': 'a', 'attribute': 'href'}
}

Extracts multiple fields at once
Returns list of dictionaries (structured data)


üéØ How to Use It
Example 1: Scrape Blog Headlines
python# Create scraper instance
scraper = WebScraper("https://example-blog.com")

# Fetch and parse
html = scraper.fetch_page()
scraper.parse_html(html)

# Extract headlines
headlines = scraper.extract_data('h2.post-title')
print(headlines)
Example 2: Scrape Product Information
pythonscraper = WebScraper("https://shop.com/products")

config = {
    'name': {'selector': '.product-name', 'attribute': 'text'},
    'price': {'selector': '.price', 'attribute': 'text'},
    'image': {'selector': 'img', 'attribute': 'src'}
}

products = scraper.scrape(selector_config=config)
scraper.save_to_csv('products.csv')

üõ†Ô∏è Advanced Features Explained
1. Session Management
pythonself.session = requests.Session()
Why? Reuses TCP connections = faster subsequent requests
2. User-Agent Headers
python'User-Agent': 'Mozilla/5.0...'
Why? Some websites block requests without proper headers (they think you're a bot)
3. Error Handling
pythontry:
    response.raise_for_status()
except requests.RequestException:
Handles: Connection errors, timeouts, 404s, 500s
4. Rate Limiting
pythontime.sleep(self.delay)

 Python Concepts Used
Concept                              How It's Used
Classes & OOP                        Encapsulating scraping logic
Type Hints                           List[Dict], Optional[str] for clarity
Exception Handling                   try/except for network errors
Context Managers                     with open() for file handling
List Comprehensions                  [x for x in elements]
String Methods                       .strip(), .get_text()
Dictionary Methods                   .keys(), .items()
Module Imports                       from bs4 import BeautifulSoup
6 Months of My AI Engineer Journey 
Week 2: Advanced Python & OOP 